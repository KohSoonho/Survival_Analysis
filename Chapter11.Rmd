---
title: 'Chapter11: Sample Size Determination for Survival Studies'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(survival)
library(survminer)
library(broom)
library(asaur)
```

Deciding how many subjects to include in a randomized clinical trial is a key component of its design. In the classical hypothesis testing framework, for any type of outcome, one must specify the effect change one is aiming for, the inherent variability in the test statistic, the significance level of the test, and the desired power of the test to detect the effect change.      
<br />

In survival analysis, there are additional factors that one must specify regarding the censoring mechanism and the particular survival distributions in the null and alternative hypotheses.     
First, one needs either to specify what parametric survival model one is using, or that the test will be semi-parametric, e.g., the log-rank test. This allows for determining the number of deaths (or events) required to meet the power and other design specifications.     
Second, one must, for administrative reasons, provide an estimate of the number of patients that need to be entered into the trial to produce the required number of deaths. We shall assume that the clinical trial is run as described in Chapter 1, where patients enter a trial over a certain accrual period of length *a*, and then followed for an additional period of time *f* known as the *follow-up time*. Patients still alive at the end of follow-up are censored. We will describe sample size methods for single arm clinical trials and then for two arm clinical trials.      

### Power and Sample Size for a Single Arm Study     

In a study with a single arm, we assume for planning purposes that the survival times follow an exponential distribution with hazard $h(t;\lambda)=\lambda$ and survival distribution $S(t, \lambda) = e^{-\lambda t}$.     
We shall test $H_0 : \lambda = \lambda_0$ versus $H_A : \lambda = \lambda_A$.    
The null hypothesis mean, $\mu_0 = 1/\lambda_0$, would correspond to the mean survival one has observed in the past for the standard therapy, and the alternative hazard, $\mu_A = 1/\lambda_A$ is a (presumably) larger mean survival that we aim to find with a new, experimental therapy.      
Thus, the treatment ratio we would like to detect may be written as $\Delta = \mu_A / \mu_0 = \lambda_0/\lambda_A$. After the trial is completed, we obtain a series ofindependentsurvivaltimes $t_1, t_2, t_3, \cdots, t_n$ and censoring indicators $\delta_1, \delta_2, \delta_3, \cdots, \delta_n$, where *n* is the total number of subjects in the trial.     
Our ultimate goal is to determine how many patients we need to detect a certain hazard ratio $\Delta$ with a specified power and significance level. The derivation of the formula is similar to that for tests concerning the mean of normally distributed observations, but some adjustments are needed to account for the presence of censoring.    
The main adjustment is that our sample size formula will directly specify $d$, the number of deaths needed to achieve the desired power. Once we have $d$, we use a separate method to find the number of patients $n$ needed to produce the required $d$.     
In practice, the null and alternative hypotheses may be expressed in terms of either median survival or survival probability at a specified time $t$. A median survival may be converted into a hazard rate by re-expressing $0.5 = e^{-\lambda t}$ as $\lambda = [\log(2)]/t$. Similarly, a survival rate $p$ at time $t$ may be written as $p=e^{-\lambda t}$, which may be expressed as $\lambda = -[\log(p)]/t$.    
These comparisons are strictly valid only if the survival distribution is exponential. However, for other survival distributions, the conversions between median survival and survival rates may provide reasonable approximations when survival rates are in the neighborhood of 50 %.      
The most direct way to derive a sample size formula is based on a Wald test, but the resulting formula varies depending on the parametrization of the likelihood. The simplest derivation uses the parameter $\theta = \log(\mu) = -\log(\lambda)$. Then we may express the log-likelihood function of Sect. 2.6 as follows:         

$$
l(\theta) = d \log(\lambda) - \lambda V= -\theta d - V e^{-\theta}
$$
By following the development in Chapter2, the m.l.e. may be shown to be $\hat{\theta} = \log(V/d)$ and $var(\hat{\theta}) \approx 1/d$, where $d = \sum \delta_i$ and $V = \sum t_i$ are the number of deaths and the total patient-time, respectively.     
We will use $\hat{\theta}$ as our test statistic, and reject $H_0$ in favor of $H_A$ if $\hat{\theta} > k$ for some constant $k$. The significant level of the test, or Type I error rate, is $\alpha = Pr(\hat{\theta} > k \ |  \ \theta = \theta_0)$.     
That is, the constant $k$ is chosen so that the probability of rejecting the null hypothesis is $\alpha$, assuming that the null hypothesis is true. Using a normalizing transformation.    
$$
Z = \frac{\hat{\theta}-\mu}{1/\sqrt{d}}
$$
and hence    
$$
k = \theta_0 + \frac{z_{\alpha}}{\sqrt{d}}
$$
Now let us consider what happens under the alternative hypothesis, where $\theta = \theta_A$. The ***power*** of the test is given by    
$$
1 - \beta = Pr(\hat{\theta} > k \ |  \ \theta = \theta_A) = Pr(Z > \frac{k-\theta_A}{1/\sqrt{d}})
$$
or equivalently,    
$$
z_{1-\beta} = -z_{\beta} = \sqrt{d} (k - \theta_A)
$$
Substituting the value of k, we have   
$$
-z_{\beta} = \sqrt{d} (\theta_0 + \frac{z_{\alpha}}{\sqrt{d}} - \theta_A)
$$
Solving for d we have   
$$
d = \frac{(z_{\beta} + z_{\alpha})^2}{(\theta_0 - \theta_A)^2} 
= \frac{(z_{\beta} + z_{\alpha})^2}{(\log{\Delta})^2}  
$$
since $\log{(\Delta)} = \log{(\lambda_0)} - \log{(\lambda_A)}$.      
This gives us **the number of deaths needed to achieve the specified power, not the number of patients**.     
This derivation method is based on the parameter estimate normalized by its standard deviation, which produces the Wald test statistic. One aspect of this test is that different parametrizations lead to somewhat different formulas for the sample size. This dependence on parametrization may be largely avoided by working directly with the likelihood ratio statistic. Now, for fixed $d$, $V = \sum{t_i}$, has a gamma distribution with index $d$ and scale parameter $\lambda$. Cox and Oakes, following Epstein and Sobel, show that      
$$
W = \frac{2d\lambda}{\hat{\lambda}} \sim \chi^2_{2d} 
$$
although this result is approximate for general censoring patterns. Under $H_0:\lambda = \lambda_0$, we need to find a constant $k$ such that    
$$
\alpha = Pr(1/\hat{\lambda}>k \  | \ \lambda = \lambda_0) = Pr(W > 2kd\lambda_0)
$$
and hence $\chi^2_{2d, \alpha} = 2kd\lambda_0$. Finally,    
$$
k=\frac{\chi^2_{2d, \alpha}}{2d\lambda_0}
$$
The power of the test is given by    
$$
1-\beta=Pr(1/\hat{\lambda}>k \ |  \ \lambda = \lambda_A) = Pr(W > 2kd\lambda_A)
$$
where 
$$
k=\frac{\chi^2_{2d, 1 - \beta}}{2d\lambda_A}
$$
Equating before Eqs, we have    
$$
\Delta = \frac{\lambda_0}{\lambda_A} = \frac{\chi^2_{2d, \alpha}}{\chi^2_{2d, 1 - \beta}}
$$
For specified $\alpha$, power $1-\beta$, and ratio $\Delta$, we may solve this for the required number of deaths, $d$.      
<br />

In R, we may compute the number of deaths using the following function:      

```{r}
exp_log_mean_death <- function(delta, alpha, power){
  z_alpha <- qnorm(alpha, lower.tail = FALSE)
  z_beta <- qnorm(1 - power, lower.tail = FALSE)
  deaths <- (z_alpha + z_beta)^2 / (log(delta))^2
  return(deaths)
}
```

We use the `qnorm` function to compute $z_{\alpha}$ and $z_{\beta}$, and the final result is the number of deaths required to detect a hazard ratio $\Delta$ with significance level $\alpha$ and power $1-\beta$.     
<br />

To use the likelihood ratio method, we first compute the hazard ratio $\Delta$ given $\alpha$, $1 - \beta$, and a specified number of deaths $d$ :    

```{r}
exp_like_ratio <- function(deaths, alpha, power){
  num <- qchisq(alpha, df = (2 * deaths), lower.tail = FALSE)
  denom <- qchisq(power, df = (2 * deaths), lower.tail = FALSE)
  delta = num / denom
  return(delta)
}
```

Here we use the `qchisq` function to compute $\chi^2_{2d, \alpha}$ and $\chi^2_{2d, 1 - \beta}$.    
<br />

To get the number of deaths d for a specified $\Delta$, we define a new function `LRD` internally which is zero at the required number of deaths:     

```{r}
exp_LR_deaths <- function(delta, alpha, power){
  LRD <- function(x, alpha, power) {
    exp_like_ratio(x, alpha, power) - delta
  }
  result <- uniroot(f = LRD, lower = 1, upper = 1000, alpha = alpha, power = power)
  return(result$root)
}
```

Suppose that we are designing a Phase II oncology trial where we plan a **5 % level (one-sided) test**, and we need **80 % power** to detect a **hazard ratio of 1.5**.     
Once the above functions have been entered into R, we can find the required number of deaths as follows:    

```{r}
# log mean method
exp_log_mean_death(1.5, 0.05, 0.8)
```

```{r}
# likelihood ratio method
exp_LR_deaths(1.5, 0.05, 0.8)
```

That is, we would need (rounding up) 38 deaths according to the log mean method (function `exp_log_mean_death`), and 37 deaths according to the likelihood ratio method (function `exp_LR_deaths`).     
The two methods give similar results over a wide range of specifications.     
<br />
<br />

### Determining the Probability of Death in a Clinical Trial     

In the previous section, we saw how to compute the required number of deaths to satisfy the power, significance, and survival difference design requirements of a trial.    
But as we have seen, in survival analysis, many subjects are still alive at the time of analysis. For administrative reasons, we usually need to specify how many patients need to be entered onto the trial, not how many will die. Thus, we need to provide an estimate of the proportion􏰌 of patients who will die by the time of analysis.    
<br />
If all patients entered at the same time, we would simply have $\pi = 1 - S(t,\lambda)$, where $t$ is he follow-up time. However, patients actually enter over an accrual period of length $a$ and then, after accrual to the trial has ended, they are followed for an additional time $f$. So a patient who enters at time $t = 0$ will have failure probability $\pi(0) = 1 - S(a+f, \lambda)$, since a patient who enters at time will have the maximum possible follow-up time $a+f$. But a patient who enters at time a, which is the end of the accrual period, will have the minimum possible follow-up time f . Thus, that patient will have failure probability $\pi(a) = 1 - S(t,f)$. This is illustrated in Fig.    

<img src = "Fig11.1.png">

If patients were entered in equal numbers at times $0$ (curve A) or $a$ (curve B) only, then the probability of death would be $(\pi(0) + \pi(a))/2$. A much more realistic scenario is that the patients enter uniformly between times $0$ and $a$, so that the patient entry follows a Uniform($0$, $a$) distribution. Then the probability of death $\pi$ is obtained by averaging over these times, so that a patient that enters at time $t$ is followed for additional time $a + f - t$. This idea may be expressed by the following integral, which uses the fact that the probability of death given the patient enters at time $t$ is $1-S(a+f-t \ | \ \lambda)$,    
$$
\pi = \int^a_0 \frac{1}{a} Pr(death | enter \ at \ time \ t) dt
$$
or just   
$$
\pi = \int^a_0 \frac{1}{a} (1-S(a+f-t \ | \ \lambda)) dt
$$
Using the variable transformation $\mu = a+f-t$ and re-arranging, we have      
$$
\pi = \frac{1}{a} \int^{a+f}_{f} S(u;\lambda)dt
$$
Since we are assuming an exponential distribution, we have $S(u;\lambda)=e^{-\lambda u}$, and we have, using basic integration,     
$$
\pi=1-\frac{1}{a}\int^{a+f}_{f}e^{-\lambda u}du=1-\frac{1}{a\lambda}\{ e^{-\lambda f}
-e^{-\lambda(a+f)} \}
$$
The following function uses this expression to compute the probability of death:     
```{r}
prob_death <- function(lambda, accrual, followup) {
  return(1 - (1 / (accrual * lambda)) * (exp(-lambda * followup) - exp(-lambda * (accrual + followup))))
}
```
Consider again our example in the previous section where we plan a single sample clinical trial with a $5 %$ significance level (one-sided) test, and we need $80 %$ power to detect a hazard ratio of $1.5$. Suppose that the null hypothesis rate is $\lambda_0=0.15$, so that the alternative hypothesis hazard rate is $\lambda_1 = \lambda_0/\Delta = 0.15/0.15 = 0.10$. We suppose now that the accrual period is $a = 2$ years and that the follow-up period is an additional $f = 3$ years.     
Previously we found that 38 deaths were needed. To obtain an estimate of the number of patients needed to produce this number of deaths, we first compute the probability of death under $H1:\lambda_1 = 0.10$.    
```{r}
prob_death(lambda = 0.1, accrual = 2, followup = 3)
```
Then the number of patients needed is approximately $38/0.3285622=115.6$, or 116 after rounding up.   
```{r}
ceiling(exp_log_mean_death(delta = 1.5, alpha = 0.05, power = 0.8)) / prob_death(lambda = 0.1, accrual = 2, followup = 3)
```
This estimate depends critically not only on the assumption of an exponential distribution, but also on the unknown value of the exponential parameter $\lambda$. Using a specific value, such as $\lambda = 0.10$, is helpful in the planning stage of the trial, since administrators will need an estimate of the number of patients that will be needed. However, to maintain the integrity of the design, it would be preferable to tie the stopping rule for the trial to the number of deaths, 38, rather than to the estimated total number of patients, 116.

### Sample Size for Comparing Two Exponential Survival Distributions     

We now consider a comparative clinical trial, where an experimental regimen is being compared to a standard, control regimen. Suppose that we are going to test the null hypothesis $H_0:S_0 \geq S_1$ versus the alternative $S_0 < S_1$ for all $t$, where $S_0$ and $S_1$ are exponential survival distributions with hazards $\lambda_0$ and $\lambda_1$, for the control and experimental regimens, respectively. To determine the required sample size, we consider the hazard ratio $\Delta = \lambda_0 / \lambda_1$.    
Now $\lambda_0$ and $\lambda_1$ are the hazards for a control and experimental treatments, and we presume that the latter hazard is the smaller one, and that our test may be viewed as the one-sided test of $H_0:\Delta = 1$ versus $H_1:\Delta > 1$. This well-known case was developed by Bernstein and Lagakos, Rubenstein et al. and others.      
We let $p$ denote the proportion of patients randomized to the control group. Typically one uses equal randomization, so that $p=0.5$, but this is not required. We denote by $n$ the total number of patients in the trial, and we have $n_0=np$ and $n_1=n(1-p)$ control and experimental patients, respectively.    
When the trial has been completed, we will observe $d_0$ and $d_1$ deaths in the control and experimental groups, and total patient times of $V_0=\sum t_{0i}$ and $V_1=\sum t_{1i}$, respectively. We know from Sect 2 that the maximum likelihood estimates of the hazards are $\hat{\lambda_0} = d_0/V_0$ and $\hat{\lambda_1} = d_1/V_1$.      
To compare the two distributions, it is more convenient to use $\delta = \log{\Delta} = \log{\lambda_0}-\log{\lambda_1}$, since the log scale transformed value will be more symmetric. One may show that, based on maximum likelihood theory,     
$$
var(\hat{\delta})=\sigma^{2}=(\frac{1}{E(d_0)}+\frac{1}{E(d_1)})=
\frac{1}{n_0\pi_0}+\frac{1}{n_1\pi_1}=\frac{1}{np(1-p)}\cdot\frac{p\pi_0+(1-p)\pi_1}{\pi_0\pi_1}
$$

where $\pi_0$ and $\pi_1$ are the probabilities of death in the control and treatment groups, respectively, If we define new parameter $\tilde{\pi}$ as follow,    
$$
\tilde{\pi}=(\frac{p\pi_0+(1-p)\pi_1}{\pi_0\pi_1})^{-1}=(\frac{p}{\pi_1}+\frac{1-p}{\pi_0})^{-1}
$$
we see that $\tilde{\pi}$ is a weighted harmonic mean of $\pi_0$ and $\pi_1$, and thus may be viewed as an average probability of death across the control and treatment groups. The harmonic mean $\tilde{\pi}$ is an approximation to the weighted mean $\tilde{\pi}=p\pi_0+(1-p)\pi_1$.     
Thus, we have    
$$
var(\hat{\delta})=\sigma^2=\frac{1}{np(1-p)}\cdot\tilde{\pi}^{-1}
$$
Expressing the test in terms of $\delta=\log{\Delta}$, we reject $H_0:\delta=0$ in favor of $H_A:\delta>0$ if $\hat{\delta}>k$ for some constant $k$. For a one-sided test, we have, following an argument similar to that in Sect. 11.1,      
$$
\alpha=Pr(\hat{\delta}>k \ | \ \delta=0)=Pr(Z>k/\alpha)
$$
where $\sigma$ is defined above. Then $k=z_{\alpha}\sigma$. The power is given by     
$$
1-\beta=Pr(\hat{\delta}>k \ | \ \delta) = Pr(Z>\frac{k-\delta}{\sigma})
$$
so, $z_{1-\beta}=-z_{\beta}=\frac{k-\delta}{\sigma}$. Substituting the value of $k$, we get $z_{\beta}=\frac{\delta}{\sigma}-z_{\alpha}$, and finally     
$$
\frac{\delta^2}{(z_{\alpha}+z_{\beta})^2}=\sigma^2=\frac{1}{np(1-p)}\cdot\tilde{\pi}^{-1}
$$
Solving for $n$, we have    
$$
n=\frac{(z_{\alpha}+z_{\beta})^2}{\delta^2p(1-p)\tilde{\pi}}
$$
This states that the required number of patients is the number of deaths,      
$$
d=\frac{(z_{\alpha}+z_{\beta})^2}{\delta^2p(1-p)}
$$
divided by the probability of death, $\tilde{\pi}$, as in Sect. 11.2.    
The only difference here is that the probability of death is an average (actually the geometric mean) of the death probabilities in the control and treatment groups (Eq. 11.3.1). The harmonic mean effectively weights the smaller of the two death probabilities more heavily than does the sample mean $\tilde{\pi_m}=(\pi_0+\pi_1)/2$. The consequence is that the harmonic mean estimate of the required sample size will be larger than the estimate one would obtain using the sample mean. We will see this in the worked examples.      
<br />

### Sample Size for Comparing Two Survival Distributions Using the Log-Rank Test     

When the data from a completed comparative clinical trial are analyzed, typically one uses a log-rank test rather than a test based on the exponential distribution, for reasons discussed in Chapter 3. Thus, it would seem reasonable to develop a sample size formula based on the log-rank test.      
With this test, which is based on the proportional hazards assumption, the ratio $\Delta = \lambda_0(t)/\lambda_1(t)$ is constant, but the baseline hazard $\lambda_0(t)$ is unspecified.     
<br />

We then use the log-rank statistics $U_0$ and its variance, which for the $i$th failure time is given by     
$$
v_{0i}=var(d_{0i})=\frac{n_{0i}n_{1i}d_i(n_i-d_i)}{n_i^2(n_i-1)}
$$
As explained in Chapter 3, these are also the score function of the partial likelihood and its variance. Assuming that the number of deaths at each failure time is small compared to the number at risk, and that the proportion $p \approx n_{01}/n_{1i}$ of subjects assigned to the control group is constant over time, we have the following approximation,    
$$
v_{0i}=var(d_{0i})=\frac{n_{0i}n_{1i}d_i(n_i-d_i)}{n_i^2(n_i-1)}
\approx \frac{n_{0i}n_{1i}d_i}{n_i^2} \approx p(1-p)d_i
$$
The variance of the log-rank statistic is then approximately     
$$
V_0=\displaystyle \sum^{D}_{i=1} v_{0i} \approx 
p(1-p)\sum^{D}_{i=1} d_i=p(1-p)d
$$
Then using a standard sample size and power calculation (see Collett and Schoenfeld for details), we find that the number of events needed to detect a treatment difference $\delta=\log(\lambda_0(t)/\lambda_1(t))=\log\Delta$ with power $1-\beta$ and with a two-sided level $\alpha$ log-rank test, is identical to that given in Eq. 11.3.2.     
$$
d=\frac{(z_{\alpha/2}+z_{\beta})^2}{p(1-p)\delta^2}
$$
or, if the same number of patients are in both groups, by     
$$
d=\frac{(z_{\alpha/2}+z_{\beta})^2}{\delta^2/4}
$$
As in previous cases, this formula gives us the required number of deaths (or events), not the number of patients. Estimating the number of patients that are needed requires and estimate of the probability of death, as we saw in Sect. 11.3.      

### Determining the Probability of Death from a Non-parametric Survival Curve Estimate       

One way of determining the number of patients needed to produce a particular number of deaths is to assume that patients enter uniformly over the accrual period, and that survival is governed by an exponential distribution. Then we can proceed as in Sect. 11.2.     
However, if a survival distribution estimate is available for the control group, say, from an earlier trial, then we can use that, along with the proportional hazards assumption, to estimate a probability of death without assuming that the survival distribution is exponential.        
Typically, the survival function for the control group of a randomized trial is a Kaplan-Meier estimate $\hat{S_0} (t)$ obtained from a prior study. If we need to detect a hazard ratio $\Delta$, the alternative hypothesis survival function will be, assuming proportional hazards,      
$$
\hat{S_1} (t) = [\hat{S_0} (t)]^{1/\Delta}
$$
Then to compute the expected number of deaths with accrual and followup times $a$ and $f$, we use the weighted mean survival,      
$$
\hat{S} (t) = p\hat{S_0} (t) + (1-p) \hat{S_1} (t)
$$
where $p$ is the proportion of subjects randomly assigned to the control group.     
Given a survival function $\hat{S}(t)$ here are a number of ways of evaluating the integral in Eq. 11.2.2.    
In most cases we may obtain a good approximation of the integral by evaluating $\hat{S}(t)$ for a patient entering at time $0$, $a/2$, and $a$, and use some results from elementary integral calculus.    
One simple approach is the trapezoidal rule, which uses the areas under two trapezoids defined by the time points $0$, $a/2$, and $a$ and values that match the integrand at these points. This yields an estimate of the probability of death given by     
$$
\pi_t \approx 1-\frac{1}{4} \{\hat{S}(a+f)+2\hat{S}(\frac{a}{2}+f)+\hat{S}(f)\}
$$
Alternatively, we can use Simpson’s rule, which uses the area under a quadratic polynomial that matches the integrand at these three points. The underlying algebra is a bit tedious, but the well-known end result is quite simple:      
$$
\pi_s \approx 1-\frac{1}{6} \{\hat{S}(a+f)+4\hat{S}(\frac{a}{2}+f)+\hat{S}(f)\}
$$
The most accurate method is to evaluate the integral numerically. Since the survival estimate $\hat{S}(t)$, a weighted mean of $\hat{S_0} (t)$ and $\hat{S_1}(t)$, is a step function, the integral may be written as a sum of areas of rectangles under each “step” at the failure times between $a$ and $a+f$.     
To do this we denote all of the ordered failure times by $t_{(1)},t_{(2)}, ...,t_{(n)}$. Then the integral in Eq. 11.2.2 may be estimated as follows:      
$$
\pi_{r}=\displaystyle \sum_{t_{(i)}:f<t_{(i)}\leq f+a} [\hat{S}(a+f-t_{(i)})\cdot(t_{(i)}-t_{(i-1)})]
$$
We may illustrate estimating $\pi$ using the data `gastricXelox`. This figure illustrates this.      
The probability of death may be computed as follows.      
<br />

First, we extract the failure times and survival probabilities:     
```{r}
tbl_gastric <- as_data_frame(gastricXelox) %>% 
  mutate(timeMonths = timeWeeks * 7 / 30.25)

gastric_km <- survfit(Surv(timeMonths, delta) ~ 1, data = tbl_gastric, conf.type = "log-log")

# extract time and surv data from gastric_km
gastric_times <- gastric_km$time
gastric_surv <- gastric_km$surv
```
Next we set up the accrual and follow-up times, and select the portion of the failure times in the interval from $f$ to $a+f$, taking care to include the time f for the first rectangle:     
```{r}
accrual <- 12
followup <- 6

# extract data between followup(6) and followup + accrual (18)
times_use <- c(followup, gastric_times[between(gastric_times, followup, followup + accrual)])
surv_use <- summary(gastric_km, times = times_use)$surv
```
Finally, we use the `diff` function to get the widths of the rectangles, and `sum` to complete the evaluation of Eq. 11.2.2:      
```{r}
times_diff <- diff(c(times_use, accrual + followup))
pi_rec <- 1 - (1 / accrual) * sum(times_diff * surv_use)
pi_rec
```
<img src = "Figure11.2.png">

Thus, we estimate, using the “rectangle” method, that $\pi_r= 0.536$.     
<br />

To do this using Simpson’s method, we first evaluate the survival function at $f$, $a/2+f$, $a+f$:       
```{r}
surv_simpson <- summary(gastric_km, 
                        times = c(followup, (accrual / 2) + followup, accrual + followup))$surv
surv_simpson
```
Then we evaluate the probability of death:     
```{r}
pi_simpson <- 1 - (1/6) * (surv_simpson[1] + 4 * surv_simpson[2] + surv_simpson[3])
pi_simpson
```
Thus, our estimate of the probability of death using this method is $\pi_s = 0.523$, which is close to the rectangle method estimate $\pi_r= 0.536$.       
However we estimate the probability of death $\pi$, the number of patients needed for the trial will be estimated by $n = d/\pi$, where $d$ is the required number of deaths.       

### Example: Calculating the Required Number of Patients for a Randomized Study of Advanced Gastric Cancer Patients     

Suppose now that we need to design a two-arm randomized clinical trial to test the effect of a new therapy to Xelox in patients with advanced gastric cancer.      
For the control arm survival distribution we use the PFS survival curve in chapter 3, and we wish to have 80 % power to detect an alternative hazard ratio $\Delta = 2$ (for an alternative experimental therapy) using a 2.5 % level one-sided log-rank test. We again assume that we will accrue patients for 12 months and follow them for an additional 6 months.    
We first determine the number of events that we require, using the following R function:

```{r}
two_arm_deaths <- function(delta, p = 0.5, alpha, pwr = 0.8) {
  z_alpha <- qnorm(alpha, lower.tail = FALSE)
  z_beta <- qnorm(1 - pwr, lower.tail = FALSE)
  num <- (z_alpha + z_beta) ^ 2
  denom <- p * (1 - p) * (log(delta)) ^ 2
  death <- num / denom 
  return(death)
}
```
The number of deaths is as follows:     
```{r}
two_arm_deaths(delta = 2, p = 0.5, alpha = 0.025, pwr = 0.8)
```
That is, we need 66 events total.      
To determine the probability of death under the alternative hypothesis, we take the survival function defined by “surv_use” in the previous section, and compute the average survival according to Eqs. 11.5.1 and 11.5.2:     
```{r}
delta <- 2
surv_alt <- surv_use^(1 / delta)
surv_avg <- 0.5 * surv_use + 0.5 * surv_alt
```
The exact estimate of the probability of death is obtained as before, using “surv_avg” in place of “surv_use”:    
```{r}
(pi_exact <- 1 - (1 / accrual) * sum(times_diff * surv_avg))
```
Thus, the probability of death is $0.430$.      
Finally, the required number of patients is given by $65.346/0.436=152.0$. Thus, we would need to enroll $152$ patients, $76$ in each arm, to meet the design specifications.         
If the full survival curve is unavailable, we may still estimate the sample size by specifying the null and alternative survival distributions in terms of median survival. We can then directly convert these into exponential parameters using $\lambda=\log2/t_m$, where $t_m$ is the median survival time.      
This approximation will be reasonable if the hazard at the median survival time is near the median of an exponential distribution. In the current example, we have $\lambda_0=\log(2)/10.3=0.0673$. Thus, the hazard for the alternative hypothesis will be $\lambda_1=0.0673/2=0.0336$.     
As discussed earlier, the required number of events under the exponential assumption is $66$, the same as we need using a log-rank test. The probability of death, however, is obtained using Eq. 11.2.3 and the harmonic mean of the probabilities in the control and treatment arms using Eq. 11.3.1.       
In R, using the function “prob_death” defined in Sect. 11.2,      
```{r}
pi_0 <- prob_death(lambda = 0.0673, accrual = 12, followup = 6)
pi_1 <- prob_death(lambda = 0.0336, accrual = 12, followup = 6)
(pi_harmonic_mean <- 1 / (0.5 / pi_0 + 0.5 / pi_1))
```
Thus, the probability of death is $0.408$, as compared to $0.430$ for the nonparametric estimate. The required number of patients under the exponential assumption is $65.346/0.408=160.2$, so we would need $162$ patients all together. This is the result according to the method in Bernstein and Lagakos, as presented in Sect. 11.3.    
This estimate is somewhat higher than the value 152 we obtained using the nonparametric approach.      
To better understand the difference, suppose that, instead of the harmonic mean $0.408$, we use the sample mean:    
```{r}
(pi_avg <- (pi_0 + pi_1) / 2)
```
Then the number of subjects we would need according to this estimate of the probability of death would be $65.346/0.434 = 150.6$ or $152$ in total. This is essentially the same as that obtained using the “nonparametric” approach, and smaller than using the harmonic mean approach. The upshot is that the estimate of the total number of patients is highly sensitive to the method of computing the probability of death.       

```{r}
# using gsDesign::nSurvival
# lambda_0 = log(2)/t_m = 0.0673
# lambda_1 = lambda_0 / 2 = 0.0336
# delta = 2
# accural = 12, follow-up = 6

library(gsDesign)
# number of deaths
nEvents(hr = .5, alpha = 0.025, beta = 0.20, ratio = 1, sided = 1)

# number of sample
nSurvival(lambda1 = log(2) / 10.3, lambda2 = (log(2) / 10.3) / 2, 
          Ts = 12 + 6, Tr = 12, ratio = 1, alpha = 0.025, beta = 0.2, sided = 1)
```

### Example: Calculating the Required Number of Patients for a Randomized Study of Patients with Metastatic Colorectal Cancer

Morse et al. reported a Kaplan-Meier plot of overall survival probabilities of 161 patients with metastatic colon cancer who had undergone metastasectomy (surgical removal of cancerous growths that have spread from the colon) in Figure 5B of the paper. The survival probabilities at 24, 36, and 48 months (2, 3, and 4 years) are, respectively, 0.76, 0.59, and 0.49.     
Suppose that we plan a randomized phase III study comparing a new therapy to placebo for these patients. We plan to carry out a 0.025 level log-rank test, and wish to have 85 % power to detect an increase in the three-year survival probability from 0.59 (the current untreated rate) to 0.75. How many patients do we need?     
<br />

First, we find the hazard ratio by solving $0.59=0.75^{\Delta}$. Taking logs, we have $\Delta=\log(0.59)/\log(0.75)=1.834$. Then the number of deaths required is about 98.      
```{r}
two_arm_deaths(delta = 1.834, p = 0.5, alpha = 0.025, pwr = 0.85)

# using gsDesign::nEvents
nEvents(hr = 1.834, alpha = 0.025, beta = 0.15, ratio = 1, sided = 1)
```
A simple function to compute the probability of death using Simpson’s rule is as follows:       
```{r}
p_death_simpson <- function(aa, ff, s) {
  # Use Simpson’s rule to approximate the probability of death assuming uniform accrual
  prob <- 1 - (1 / 6) * (s[1] + 4 * s[2] + s[3])
  return(prob)
}
```
where “aa” is the accrual period, “ff” is the follow-up period, and “S” is a vector with three elements corresponding to the survival probabilities at times “ff”, “ff + 0.5 * aa”, and “ff + aa”. We define the parameters as follows:      
```{r}
aa <- 2
ff <- 2
s_obs <- c(0.76, 0.59, 0.49)
psi <- 1.834
s_a <- s_obs ^ (1 / psi)
s_both <- 0.5 * (s_obs + s_a)
```
The probabilities of death in the control arm, treatment arm, and the average, are as follows:     
```{r}
list(s_obs, s_a, s_both) %>% 
  set_names(c("p_death", "p_treat", "p_both")) %>% 
  map(~ p_death_simpson(aa = 2, ff = 2, s = .))
```
Thus, the total number of patients necessary to produce and expected value of 97.63 deaths is $97.63 / 0.3209381 = 304.202$, or $304 / 2 = 152$ patients per arm.    
<br />

### Using Simulations to Estimate Power     

The previous sections described methods for computing sample size and power for specific situations for which explicit formulas are available.     
An alternative approach to estimating power is to simulate a large number of survival data sets from a particular distribution and accrual pattern, and empirically compute the power.      
<br />
Specifically, suppose that we are computing power for a two arm randomized clinical trial comparing a standard therapy to an experimental therapy.     
Based on past studies of the standard therapy, we may select a parametric distribution that approximates the survival of patients given the standard therapy. We also specify a hazard ratio that we would like to detect.   
Then, we model the entry of patients over a specified accrual period, randomization to either of the two arms, and follow them until death or, for those still alive at the end of a pre-specified follow-up period, until they are censored. We then compute a test statistic and p-value using, typically, a log-rank test.      
We repeat this process a large number of times, and observe the proportion of times we reject the null hypothesis of no treatment difference. This is the estimated power.      
<br />

*Example 11.1.*    
Let us design a clinical trial to determine if an experimental agent can increase the time to death from prostate cancer among patients diagnosed with advanced localized prostate caner.       
The first step is to specify the eligibility criteria, and to use the data in `prostateSurvival` to determine the survival distribution (defined as time to death from prostate cancer) to select a Weibull distribution that matches the data. Specifically, we shall consider men aged 66 to 74 with newly diagnosed, poorly differentiated, stage T2 prostate cancer, and find a Weibull distribution that matches the survival proportions of these patients at four and eight years.
<br />
In R, we first define the population of interest, `prost_66_to_74_poor`, and then define a censoring variable “status_prost” that indicates death from prostate cancer. (Patients who die of other causes are here considered censored, as are patients still alive at the last time of follow-up.)     
```{r}
prost_66_to_74_poor <- prostateSurvival %>% 
  as_data_frame() %>% 
  filter(grade == "poor", ageGroup %in% c("70-74", "66-69"), stage == "T2") %>% 
  mutate(status_prost = if_else(status == 1, 1, 0))

head(prost_66_to_74_poor)
```
We obtain a Kaplan-Meier survival distribution for these data, and find the survival probabilities at 48 and 96 months (that is, at four and eight years), as follows:     
```{r}
km_prost_66_to_74_poor <- survfit(Surv(survTime, status_prost) ~ 1, data = prost_66_to_74_poor)
summary(km_prost_66_to_74_poor, time = c(48, 96))
```
Next, we find a Weibull distribution that matches the survival probabilities at these two times.    
While we could use the methods described in Chapter 10, it will be more convenient to use some facilities in the R package `Hmisc` developed byFrank Harrell.      
This package must be downloaded and installed separately. Then we define the Weibull function using the `Weibull2` function as follows:     
```{r}
library(Hmisc)
Weib_p <- Weibull2(c(4, 8), c(0.931, 0.717))
```
This creates a function named “Weib_p” that computes, for any time, the survival probability based on the Weibull distribution that we have specified. We may take a peek at it as follows:
```{r}
Weib_p
```
The parameters that “Weibull2” has computed are clearly indicated. (The parametrization used in the Hmisc package is somewhat different than what we have used in this text, but that doesn’t matter, since the associated survival functions in this package are consistent with this.)        
Next, we define a set of two functions using the Hmisc function `Quantile2`, which allows us to specify the control survival function (“Weib_p”) and the hazard ratio. The hazard ratio here is assumed to be a constant, 0.75, which would indicate that the experimental agent would reduce the hazard of prostate cancer mortality by 25 %. (It is defined as a function because `Quantile2` allows specification of more complex hazard ratio relationships.)       
The result, the R object “ff”, specifies the control and experimental survival distributions. There is a plot “method” for “ff”, which means that the ordinary “plot” function will plot the survival distributions for both the control and experiment arms. Here is the R code:        
```{r}
ff <- Quantile2(Weib_p, hratio = function(x) {0.75})
plot(ff, xlim=c(0,8))
```
Before we can carry out the simulation, we need to specify names for the two survival distributions and extract them from “ff” (above Fig).    
```{r}
rcontrol <- function(n) {ff(n, what = "control")} 
rintervention <- function(n) {ff(n, what = "intervention")}
```
We also need to specify the censoring distribution, and to do this, we have to select the accrual and follow-up times. Here, let us accrue patients over three years, and follow them for an additional seven years.      
For now we shall assume that the accrual will follow a uniform distribution. This leads to the censoring distribution being uniform, with a minimum of five years (for a patient entering at the end of the accrual period) and a maximum of eight years (for a patient entering at the start of the trial).     
We specify the censoring distribution “rcens” using the R function `runif` as follows (using time in years):    
```{r}
rcens <- function(n) {runif(n, 5, 8)}
```
We carry out the power simulation using the Hmisc function `spower`.     
Here we specify that there will be nc = 1500 patients enrolled in the control arm and ni = 1500 in the intervention arm. We will simulate nc = 1000 data sets, and carry out a logrank test at the 0.025 significance level, as follows:         
```{r}
spower(rcontrol, rintervention, rcens, 
       nc = 1500, ni = 1500, test = logrank, nsim = 1000, alpha = 0.025)
```
The result of the function, 0.827, is the power of the test.    
We can verify the significance level of the test by specifying “rcontrol” as both the control and intervention distribution,     
```{r}
spower(rcontrol, rcontrol, rcens, 
       nc = 1500, ni = 1500, test = logrank, nsim = 1000, alpha = 0.025)
```
We see that the empirical Type I error rate is about 2.8 %, which is consistent with a 2.5 % level test.       
<br />
Computing power using simulations has the advantage that it can accommodate deviations from the usual assumptions of uniform accrual, proportional hazards, and perfect patient compliance.     
The `spower` function, combined with `Quantile2`, can model a wide variety of such deviations. For example, suppose that we expect that 10 % of the patients on the intervention arm will be non-compliant, in that they to not take the experimental agent. We may include that noncompliance factor in the simulation via the “dropout” argument in the `Quantile2` function.     
```{r}
ff_dropout <- Quantile2(Weib_p, hratio = function(x) {0.75}, 
                        dropout = function(x) {0.10})

r_control_dropout <- function(n) {ff_dropout(n, what = "control")}
r_intervention_dropout <- function(n) {ff_dropout(n, what = "intervention")}

spower(r_control_dropout, r_intervention_dropout, rcens, 
       nc = 1500, ni = 1500, test = logrank, nsim = 1000, alpha = 0.025)
```
We see that the noncompliance has resulted in a loss of power, from 82.7 % to 73.4 %.       
<br />
The `spower` suite of functions can accommodate a wide variety of deviations, including non-uniform accrual, non-proportional hazards, and noncompliance in either the control or intervention subjects. Details and examples may be found in the R help file for `spower` in the `Hmisc` package.     
If one needs to find the sample size or detectable hazard ratio for a specific power (80 % for example), one can use trial and error.     
Alternatively, one can define an R function that takes (for example) the hazard ratio as an argument and returns the power. Using that, combined with the R function `uniroot`, one can solve to get the detectable hazard ratio.
     
### Additional Notes    
1. Freedman derived an alternative to Eq.11.4.2 for the number of deaths required using the log-rank test:     
$$
d = \frac{(z_{\alpha/2}+z_{\beta})^2(\Delta + 1)^2}{(\Delta-1)^2}
$$
This is approximately equal to the number of deaths given in Eq. 11.4.2.      
To see this, let $\psi = (\Delta+1)/(\Delta-1)$. Using the first term of a standard logarithm series expansion, we have $\log(\Delta)\approx 2\psi$. Substituting into Eq. 11.9.1 we get Eq. 11.4.2. See for more details.     
<br />
2. Methods for testing for non-inferiority, bio-equivalence, adaptive sample size methods, and the use of alpha spending functions for interim analyses have been adapted to use survival data.    
See Shih and Aisner for a review of these methods.     

### Exercises      

1. For the colon cancer example in Sect.11.7, compute the probability of death and required number of patients assuming an exponential distribution with a three year survival probability of 0.59.    

```{r}
# needed number of deaths
two_arm_deaths(delta = 1.834, p = 0.5, alpha = 0.025, pwr = 0.85)

```

2. Using the exponential distribution from Exercise 1, find the survival probabilities at 2, 3, and 4 years, and use them in the Simpson’s rule method to obtain the probability of death. Compare your answers to those given in Exercise 1.      

```{r}

```


3. onsider the prostate cancer clinical trial of Example 11.1, where there was a 10 % non-compliance rate. We found that we had 73.4 % power to detect a hazard ratio of 0.75. What would the hazard ratio need to be to be detactable with 80 % power?     

```{r}

```
